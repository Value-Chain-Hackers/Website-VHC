---
title: "Supply Chain Mapping"

---

## Map Supply Chains using ontologies


Extracting knowledge from large text documents is not a single step.
To retrieve valuable information from these a large document or corpus needs to be chuncked and the task needs to be subdivided in multiple sub-tasks.

```{mermaid}
graph LR
style PDF fill:#008000, stroke:#000000
style Crawling fill:#ffa500, stroke:#000000
style SEC fill:#ff0000, stroke:#000000
style Wikipedia fill:#ffa500, stroke:#000000
style DuckDuckGo fill:#ffa500, stroke:#000000
style Store fill:#ffff00, stroke:#000000
style Ingestion fill:#008000, stroke:#000000
style Transform fill:#ffff00, stroke:#000000
style Extract fill:#008000, stroke:#000000
style LLM fill:#008000, stroke:#000000
style IndexGraph fill:#ffa500, stroke:#000000
style IndexFact fill:#ffa500, stroke:#000000
style Neo4j fill:#ffa500, stroke:#000000
style Sqlite fill:#ffff00, stroke:#000000
style Markdown fill:#008000, stroke:#000000
style Faiss fill:#ffa500, stroke:#000000
style Chroma fill:#008000, stroke:#000000
style RAG fill:#ffff00, stroke:#000000
style UIAI fill:#ffa500, stroke:#000000
style Query fill:#ffa500, stroke:#000000
style Retrievers fill:#ffa500, stroke:#000000
style Results fill:#ffa500, stroke:#000000
style Visualize fill:#ffa500, stroke:#000000
style Embeddings fill:#ffa500, stroke:#000000
style Structurize fill:#ffa500, stroke:#000000

subgraph Collect
    PDF[PDF\nannual reports] --> Store
    Crawling[Crawling:\nWeb Site\nBrand Site\nProduct Pages] --> Store
    SEC[US SEC\nFilings\n10-K\n10-Q] --> Store
    Wikipedia[Wiki\nCompany Pages\nIngredients\nChemicals] --> Store
    DuckDuckGo[Search\nDuckDuckGo] --> Store
    Store --> Ingestion[Processing\nIngestion]
end

subgraph Ingest
    Ingestion --> Transform <--> Extract[Extract:\nNodes, Edges]
    Transform <--> Structurize[Structurize:\nCSV, JSON, Markdown]
    Transform --> Embeddings
    Structurize --> Knowledge
end

subgraph Knowledge
    Transform --> LLM
    Extract <--> IndexGraph
    Extract <--> IndexFact
    LLM --> Structurize
end

RAG --> Retrievers

subgraph GraphDatabase
    IndexGraph[Graph\nNodes\nLinks] --> Neo4j 
    RAG --> Neo4j
end

subgraph FactDatabase
    IndexFact --> Sqlite 
    IndexFact --> Markdown
    Markdown --> RAG
    Sqlite --> RAG
end

subgraph VectorStores
    Embeddings --> Faiss
    Embeddings --> Chroma
    Faiss --> RAG
    Chroma --> RAG
end

subgraph UI
    UIAI --> Query --> Retrievers --> Results --> Visualize
    Query --> Visualize
end

```

## Chunks : Chunkin a large document in pieces

This step will take a large document and chuck it in parts using some overlap to ensure that paragraph that would be cut in their middle are also present in the next chunk of text.

## Preprocess : Remove the grease

This step will tranform the larger text chunks to sentences that are focused on facts, named entities, take-aways and essential points removing the grease of what is not relevant to the extraction process, but present in the textual form of the document.

[Pre-Process Prompt](./Prompt_Extraction_preprocess.qmd)

## Extract : Get tuples of knowledge from the text

With the reduced text now the model will attempt to make tuples and nodes according to the ontology. Here too `divide and conquer` will be used, in order to reduce hallucianations it's better to invoke the model multiple times using a subset of the ontlogy each time. This will yield better results and avoid the model to try to makeup nodes for each class of the ontology.

[Extracting Prompt](./Prompt_Extraction.qmd)

## Validate : Check facts, deduplicate ensure proper syntax

Now we have extracted nodes, we will invoke antoher model to do a quality check on each node, by adding a rag on the actual document this step is able to search back in the document and coroborate different pieces of the document to make the most complete set of tuples and nodes, suppressing normally any hallucinated facts that might have produced by previous steps.

[Validation Prompt](./Prompt_Extraction_validation.qmd)

## Ingest : Incorporate tuples and nodes to the global graph

With our appured nodes being now filtered and checked for quality we will now query the graph for each of them and add them if they are not a perfect duplicate and reject the nodes that might be too similar using cosine similarity and clustering techniques.

## Technical Budget

[technical budget](./technicalbudget.qmd)